# MoE Expert Memory Layout Analysis

## Key Finding: Unified Expert Layout

**All quantized MoE models in tinygrad use the same `expert_first_in_memory` layout.**

### Test Results

Tested models using `Transformer.from_gguf()` loading (see `check_expert_layout.py`):

| Model | expert_first_in_memory | Layout | Expert Weights |
|-------|----------------------|--------|----------------|
| deepseek-v2-lite | `True` | expert-contiguous | 52 instances |
| glm-4.7:flash | `True` | expert-contiguous | 138 instances |
| youtu-llm:2b-Q4 | N/A | N/A | Not a MoE model |

### What This Means

Both DeepSeek-V2-Lite and GLM-4.7-Flash use **expert-contiguous** memory layout:
```
Memory: [expert0_blocks][expert1_blocks][expert2_blocks]...
```

This is determined automatically in `tinygrad/nn/state.py:543` by analyzing GGUF dimension order:
```python
expert_first_in_memory = (expert_dim_idx == 2)  # expert dim is last in GGUF = expert-first in memory
```

### Simplification Opportunity

Since all current MoE models use `expert_first_in_memory=True`:

1. **The conditional branch in `QuantizedExpertWeights._reshape_blocks()` is currently unused**
   - Lines 388-400 in `tinygrad/apps/llm.py`
   - The `else` branch (row-interleaved layout) never executes

2. **The parameter in fused kernels is always `False`**
   - Lines 415, 418, 421: `not self.expert_first_in_memory` always evaluates to `False`
   - Example: `q4k_moe_fused(..., not self.expert_first_in_memory)` â†’ `q4k_moe_fused(..., False)`

### Recommendation

**Keep the code as-is for now:**
- The layout flag is derived from GGUF metadata, not hardcoded per model
- Different quantization formats or future models might use row-interleaved layout
- The conditional adds minimal overhead (single boolean check)
- Code correctly handles both layouts per GGML spec

The abstraction is sound even if only one path is currently exercised. Removing it would save no measurable performance and would require reinstating if a row-interleaved model appears.

### How Expert Layout is Detected

From `tinygrad/nn/state.py:532-543`:

```python
# MoE expert tensors: detect layout from GGUF dims, not just name
# GGUF column-major: dims[0] = fastest varying, dims[-1] = slowest varying
# If expert count (small dim, typically 64) is dims[-1], data is expert-contiguous
# If expert count is dims[0], data is row-interleaved
is_expert_tensor = any(x in name for x in ['_exps', '_shexp'])
expert_first_in_memory = True  # default: expert-contiguous
if is_expert_tensor and len(dims) == 3:
  # Find the expert dimension (smallest of the 3 dims, typically 64)
  expert_dim_idx = min(range(3), key=lambda i: dims[i])
  # If expert is dims[-1] (slowest varying), it's expert-contiguous in memory
  # If expert is dims[0] (fastest varying), it's row-interleaved
  expert_first_in_memory = (expert_dim_idx == 2)
```

The layout depends on how the model was quantized/exported to GGUF, not on the model architecture itself.

### Example: DeepSeek-V2-Lite Expert Weights

```
blk[1].ffn_gate_exps:
  expert_first_in_memory: True
  num_experts: 64
  shape: (1408, 2048)  # (out_features, in_features)

blk[1].ffn_up_exps:
  expert_first_in_memory: True
  num_experts: 64
  shape: (1408, 2048)
```

Each layer has 2 expert weight tensors (gate and up projections), with 64 experts per tensor.

---

*Generated by `check_expert_layout.py` on 2026-02-03*

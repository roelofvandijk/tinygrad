# Claude Code Guide for tinygrad

## Architecture Overview

tinygrad compiles tensor operations into optimized kernels. The pipeline:

1. **Tensor** (`tensor.py`) - User-facing API, creates UOp graph
2. **UOp** (`uop/ops.py`) - Unified IR for all operations (both tensor and kernel level)
3. **Schedule** (`engine/schedule.py`, `schedule/`) - Converts tensor UOps to kernel UOps
4. **Codegen** (`codegen/`) - Converts kernel UOps to device code
5. **Runtime** (`runtime/`) - Device-specific execution

## Key Concepts

### UOp (Universal Operation)
Everything is a UOp - tensors, operations, buffers, kernels. Key properties:
- `op`: The operation type (Ops enum)
- `dtype`: Data type
- `src`: Tuple of source UOps
- `arg`: Operation-specific argument
- `tag`: Optional tag for graph transformations

UOps are **immutable and cached** - creating the same UOp twice returns the same object (ucache).

### PatternMatcher
Used extensively for graph transformations:
```python
pm = PatternMatcher([
  (UPat(Ops.ADD, src=(UPat.cvar("x"), UPat.cvar("x"))), lambda x: x * 2),
])
result = graph_rewrite(uop, pm)
```

### Schedule Cache
Schedules are cached by graph structure. BIND nodes (variables with bound values) are unbound before cache key computation so different values hit the same cache.

## Testing

```bash
# Run specific test
python -m pytest test/unit/test_schedule_cache.py -xvs

# Run with timeout
python -m pytest test/test_symbolic_ops.py -x --timeout=60

# Debug with print
DEBUG=2 python -m pytest test/test_schedule.py::test_name -xvs

# Visualize UOp graphs
VIZ=1 python -c "from tinygrad import Tensor; Tensor.ones(10).sum().realize()"
```

## Common Environment Variables

- `DEBUG=1-7` - Increasing verbosity (7 shows assembly output)
- `VIZ=1` - Enable graph visualization
- `SPEC=1` - Enable UOp spec verification
- `NOOPT=1` - Disable optimizations
- `DEVICE=CPU/CUDA/AMD/METAL` - Set default device

## Debugging Tips

1. **Print UOp graphs**: `print(tensor.uop)` or `print(tensor.uop.sink())`
2. **Check schedule**: `tensor.schedule()` returns list of ExecItems
3. **Trace graph rewrites**: Use `VIZ=1` or add print in PatternMatcher callbacks
4. **Find UOps by type**: `[u for u in uop.toposort() if u.op is Ops.SOMETHING]`

## Workflow Rules

- **NEVER commit without explicit user approval** - always show the diff and wait for approval
- **NEVER amend commits** - always create a new commit instead
- Run `pre-commit run --all-files` before committing to catch linting/type errors
- Run tests before proposing commits
- Test with `SPEC=2` when modifying UOp-related code

## Auto-generated Files (DO NOT EDIT)

The following files are auto-generated and should never be edited manually:
- `extra/assembly/amd/autogen/{arch}/__init__.py` - Generated by `python -m extra.assembly.amd.dsl --arch {arch}`
- `extra/assembly/amd/autogen/{arch}/gen_pcode.py` - Generated by `python -m extra.assembly.amd.pcode --arch {arch}`

Where `{arch}` is one of: `rdna3`, `rdna4`, `cdna`

To add missing instruction implementations, add them to `extra/assembly/amd/emu.py` instead.

## Style Notes

- 2-space indentation, 150 char line limit
- **Code should be dense, tasteful, minimal, clean, and correct** - no cruft, no over-engineering, no unnecessary abstractions
- PatternMatchers should be defined at module level (slow to construct)
- Prefer `graph_rewrite` over manual graph traversal
- UOp methods like `.replace()` preserve tags unless explicitly changed
- Use `.rtag(value)` to add tags to UOps

## Lessons Learned

### UOp ucache Behavior
UOps are cached by their contents - creating a UOp with identical (op, dtype, src, arg) returns the **same object**. This means:
- `uop.replace(tag=None)` on a tagged UOp returns the original untagged UOp if it exists in cache
- Two UOps with same structure are identical (`is` comparison works)

### Spec Validation
When adding new UOp patterns, update `tinygrad/uop/spec.py`. Test with:
```bash
SPEC=2 python3 test/unit/test_something.py
```
Spec issues appear as `RuntimeError: SPEC ISSUE None: UOp(...)`.

### Schedule Cache Key Normalization
The schedule cache strips values from BIND nodes so different bound values (e.g., KV cache positions) hit the same cache entry:
- `pm_pre_sched_cache`: BIND(DEFINE_VAR, CONST) → BIND(DEFINE_VAR) for cache key
- `pm_post_sched_cache`: restores original BIND from context
- When accessing `bind.src[1]`, check `len(bind.src) > 1` first (might be stripped)
- Extract var_vals from `input_buffers` dict after graph_rewrite (avoids extra toposort)

### Avoiding Extra Work
- Use ctx dict from graph_rewrite to collect info during traversal instead of separate toposort
- Only extract var_vals when schedule is non-empty (no kernels = no vars needed)
- PatternMatchers are slow to construct - define at module level, not in functions

### Readability Over Speed
Don't add complexity for marginal performance gains. Simpler code that's slightly slower is often better:
```python
# BAD: "optimized" with extra complexity
if has_afters:  # skip toposort if no AFTERs
  after_map = [(u, u.buf_uop) for u in big_sink.toposort() if u.op is Ops.AFTER]

# GOOD: simple, always works
after_map = [(u, u.buf_uop) for u in big_sink.toposort() if u.op is Ops.AFTER]
```
The conditional check adds complexity, potential bugs, and often negligible speedup. Only optimize when profiling shows a real bottleneck.

### Testing LLM Changes

**IMPORTANT RULES:**
- **NEVER use `head` or `tail` when running model inference ** - these commands are expensive. Always write full output to a file.
- **NEVER pipe echo into llm.py** - always use `--prompt` and `--count` flags instead.
- **Write output files to this folder** - use relative paths like `./llm_test.log`, not `/tmp/`.

```bash
# Quick smoke test (correct way)
DEBUG=1 python tinygrad/apps/llm.py --model "llama3.2:1b" --prompt "Hello" --count 10 > ./llm_test.log 2>&1

# Test DeepSeek-V2-Lite
DDEBUG=1 python tinygrad/apps/llm.py --model deepseek-v2-lite --prompt "User: Hi\n\nAssistant:" --count 5 > ./ds2_test.log 2>&1

# Check cache hits (read the log file, don't grep)
DEBUG=1 python tinygrad/apps/llm.py --model "llama3.2:1b" --prompt "Hello world" --count 5 > ./cache_test.log 2>&1

# Test with beam search
BEAM=2 python tinygrad/apps/llm.py --model "llama3.2:1b" --prompt "Hello" --count 10 > ./beam_test.log 2>&1
```

## Common Patterns

### Graph Transformation
```python
def my_transform(ctx, x):
  # Return new UOp or None to skip
  return x.replace(arg=new_arg)

pm = PatternMatcher([
  (UPat(Ops.SOMETHING, name="x"), my_transform),
])
result = graph_rewrite(input_uop, pm, ctx={})
```

### Finding Variables
```python
# Get all variables in a UOp graph
variables = uop.variables()

# Get bound variable values
var, val = bind_uop.unbind()
```

### Shape Handling
```python
# Shapes can be symbolic (contain UOps)
shape = tensor.shape  # tuple[sint, ...] where sint = int | UOp
```

## Performance Optimization

When optimizing tinygrad internals:

1. **Measure wall time, not just call counts** - Reducing `graph_rewrite` calls doesn't always improve wall time. The overhead of conditional checks can exceed the cost of the operation being skipped.

2. **Profile each optimization individually** - Run benchmarks with and without each change to measure actual impact. Use `test/external/external_benchmark_schedule.py` for schedule/rewrite timing.

3. **Early exits in hot paths are effective** - Simple checks like `if self.op is Ops.CONST: return self` in `simplify()` can eliminate many unnecessary `graph_rewrite` calls.

4. **`graph_rewrite` is expensive** - Each call has overhead even for small graphs. Avoid calling it when the result is trivially known (e.g., simplifying a CONST returns itself).

5. **Beware iterator overhead** - Checks like `all(x.op is Ops.CONST for x in self.src)` can be slower than just running the operation, especially for small sequences.

6. **Verify cache hit rates before adding/keeping caches** - Measure actual hit rates with real workloads. A cache with 0% hit rate is pure overhead (e.g., `pm_cache` was removed because the algorithm guarantees each UOp is only passed to `pm_rewrite` once).

7. **Use `TRACK_MATCH_STATS=2` to profile pattern matching** - This shows match rates and time per pattern. Look for patterns with 0% match rate that still cost significant time - these are pure overhead for that workload.

8. **Cached properties beat manual traversal** - `backward_slice` uses `@functools.cached_property`. A DFS with early-exit sounds faster but is actually slower because it doesn't benefit from caching. The cache hit benefit often outweighs algorithmic improvements.

9. **Avoid creating intermediate objects in hot paths** - For example, `any(x.op in ops for x in self.backward_slice)` is faster than `any(x.op in ops for x in {self:None, **self.backward_slice})` because it avoids dict creation.

## Pattern Matching Analysis

**Use the right tool:**

- `TRACK_MATCH_STATS=2` - **Profiling**: identify expensive patterns
- `VIZ=-1` - **Inspection**: see all transformations, what every match pattern does, the before/after diffs

```bash
TRACK_MATCH_STATS=2 PYTHONPATH="." python3 test/external/external_benchmark_schedule.py
```

Output format: `matches / attempts -- match_time / total_time ms -- location`

Key patterns to watch (from ResNet50 benchmark):
- `split_load_store`: ~146ms, 31% match rate - does real work
- `simplify_valid`: ~75ms, 0% match rate in this workload - checks AND ops for INDEX in backward slice
- `vmin==vmax folding`: ~55ms, 0.33% match rate - checks 52K ops but rarely matches

Patterns with 0% match rate are workload-specific overhead. They may be useful in other workloads, so don't remove them without understanding their purpose.

```bash
# Save the trace
VIZ=-1 python test/test_tiny.py TestTiny.test_gemm

# Explore it
./extra/viz/cli.py --help
```

## AMD Performance Counter Profiling

Set VIZ to `-2` to save performance counters traces for the AMD backend.

Use the CLI in `./extra/sqtt/roc.py` to explore the trace.

## LLM Apps How-To Guide

### Model Loading

#### From URL (Recommended)
```python
from tinygrad import Tensor
from tinygrad.apps.llm import Transformer, SimpleTokenizer, models

# Load from predefined models dict
model, kv = Transformer.from_gguf(Tensor.from_url(models["glm-4.7:flash"]), max_context=4096, quantized=True)
tok = SimpleTokenizer.from_gguf_kv(kv)
```

#### Cache Location
Downloaded models are cached at:
- macOS: `~/Library/Caches/tinygrad/downloads/`
- Linux: `~/.cache/tinygrad/downloads/`

Cache filename is MD5 hash of the URL:
```python
import hashlib
url = "https://huggingface.co/..."
cache_name = hashlib.md5(url.encode('utf-8')).hexdigest()
```

#### Pre-populating Cache
To avoid re-downloading, move existing GGUF files to cache with correct hash name:
```bash
# Get the hash for a URL
python -c "import hashlib; print(hashlib.md5('https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/resolve/main/GLM-4.7-Flash-Q4_K_M.gguf'.encode()).hexdigest())"
# 946c72affc2a08d1db03146bcb52c03a

# Move file to cache
mv GLM-4.7-Flash-Q4_K_M.gguf ~/Library/Caches/tinygrad/downloads/946c72affc2a08d1db03146bcb52c03a
```

#### Reading GGUF Metadata from Cache
```python
from tinygrad.nn.state import gguf_load
# Load directly from cache path (hash of URL)
kv, weights, quantized = gguf_load('/Users/you/Library/Caches/tinygrad/downloads/<hash>')

# View expert config
for k, v in sorted(kv.items()):
  if 'expert' in k.lower():
    print(f"{k}: {v}")
```

### Chat Templates & Presets

Each model uses a preset that defines chat formatting:
- `llama3`, `llama-v3`, `llama-bpe`: Llama-style `<|start_header_id|>role<|end_header_id|>`
- `qwen2`: Qwen-style `<|im_start|>role`
- `glm4`: GLM-style `<|user|>`, `<|assistant|>` with `<sop>` prefix
- `deepseek-llm`: Simple `User: ` / `Assistant: ` format
- `youtu`: Youtu-style `<|User|>`, `<|Assistant|>`
- `olmo`: OLMo-style `<|user|>`, `<|assistant|>`

#### Thinking/Reasoning Models
Some models support thinking mode with `<think>` tags:

| Model | Thinking Mode | How to Enable |
|-------|--------------|---------------|
| GLM-4.7 | Yes | Add `<think>\n` after `<\|assistant\|>` |
| Youtu-LLM | Yes | Add `<think>\n` after `<\|Assistant\|>` (or model generates naturally) |
| DeepSeek-V2-Lite | No | Simple chat format, no thinking |

The `SimpleTokenizer.build_chat_ids()` method automatically adds `<think>\n` for glm4 and youtu presets.

#### Extracting Chat Template from GGUF
```python
from tinygrad import Tensor, nn
kv, _, _ = nn.state.gguf_load(Tensor.from_url(url).to(None))
template = kv.get('tokenizer.chat_template', None)
print(template)
```

### LLM Troubleshooting

#### Wrong Output / Model Not Following Instructions
1. **Check chat formatting**: Raw prompts vs chat-formatted prompts produce different results
   ```bash
   # With chat formatting (recommended)
   python llm.py --model "glm-4.7:flash" --prompt "Hello"
   # Raw prompt (no chat template)
   python llm.py --model "glm-4.7:flash" --prompt "Hello" --raw-prompt
   ```
2. **Check thinking mode**: GLM4 without `<think>` prefix answers differently

#### Model Produces Garbage
1. **Check tokenizer preset**: Verify `tokenizer.ggml.pre` in GGUF matches expected preset
2. **Check BOS token**: Some models need BOS, others don't
   ```python
   add_bos = kv.get('tokenizer.ggml.add_bos_token', True)
   bos_id = kv.get('tokenizer.ggml.bos_token_id') if add_bos else None
   ```

#### Memory Issues
1. Use `quantized=True` for large MoE models (GLM, DeepSeek)
2. Reduce `max_context` to lower memory usage
3. Models are auto-quantized if they start with "glm-" or "deepseek-"

#### Slow First Token
Due to model loading, JIT compilation, and RoPE precomputation. Subsequent tokens use cached kernels.

### Architecture Notes

#### MLA (Multi-head Latent Attention)
Used by DeepSeek-V2 and GLM-4.7 models. Key indicators:
- `kv_lora_rank > 0` in GGUF metadata
- Uses compressed KV cache (smaller memory footprint)
- Has `attn_kv_a_mqa`, `attn_kv_b` weight naming

#### MoE (Mixture of Experts)
Used by OLMoE, GLM-4.7, DeepSeek-V2-Lite. Key indicators:
- `expert_count > 0` in GGUF metadata
- Weight names contain `_exps` (e.g., `ffn_gate_exps`)
- `expert_used_count` defines how many experts per token

#### YaRN (Yet another RoPE extensioN)
Extended context via RoPE frequency scaling. Key indicators:
- `rope.scaling.type == "yarn"` or `rope.scaling.factor > 1`
- Uses `precompute_freqs_cis_yarn()` for frequency computation

### Quick Profile (Recommended)

Use `profile_model.py` for a single-command overview of performance, kernel count, scheduling, and hotspots:

```bash
# Full profile report
.venv2/bin/python3 profile_model.py deepseek-v2-lite          # default 10 tokens
.venv2/bin/python3 profile_model.py glm-4.7:flash 10
.venv2/bin/python3 profile_model.py youtu-llm:2b-Q4 20

# With extra env vars (e.g. toggle features)
.venv2/bin/python3 profile_model.py deepseek-v2-lite 10 MOE_ADDS=0
```

Runs `DEBUG=2 PROFILE=1` under the hood, parses both the debug log and the profile pickle, and produces:
1. **Performance**: per-token ms/tok/s, steady-state average
2. **Kernel count**: total per token, ICB breakdown, avg overhead per kernel
3. **Scheduling**: cache hit/miss stats, per-token schedule patterns
4. **Warm-up kernel analysis**: top kernels by total time and call count (real pre-JIT times)
5. **Categories**: elementwise vs reduction vs q4k etc., with per-token estimates
6. **Steady-state GPU time**: per-ICB-batch timing from PROFILE pickle
7. **File paths**: raw log, profile pickle, rewrites pickle, and copy-paste commands to dig deeper

#### NOTE on timing
- **Warm-up times** (DEBUG=2, pre-JIT): REAL per-kernel times. Best proxy for individual kernel efficiency.
- **Steady-state times** (PROFILE pickle): Per-ICB-batch GPU times are real. Individual kernel times WITHIN an ICB are evenly divided across all kernels (artifact of `collect_timestamps()` in `metal.py`).

### Profiling Slow Kernels (VIZ=-1)

Use VIZ=-1 to capture rewrite traces (and it implicitly enables profiling):

```bash
# Capture
VIZ=-1 .venv2/bin/python3 tinygrad/apps/llm.py --model "deepseek-v2-lite-Q4_0" --benchmark 10

# View slowest kernels
PYTHONPATH=. .venv2/bin/python3 extra/viz/cli.py --profile --device METAL

# Inspect a specific kernel
PYTHONPATH=. .venv2/bin/python3 extra/viz/cli.py --profile --device METAL --kernel "<kernel_name>"
```

### Kernel Analysis with DEBUG=5

DEBUG=5 prints the **Metal source code** for every kernel. Essential for understanding why a kernel is slow.

```bash
DEBUG=5 .venv2/bin/python tinygrad/apps/llm.py --model "deepseek-v2-lite-Q4_0" --benchmark 3 > ./debug5.log 2>&1
```

What to look for in kernel source:
- **Grid dims**: `gidx0`, `gidx1` → workgroups. Need ~2000+ for bandwidth saturation on Apple Silicon.
- **Threadgroup dims**: `lidx0`, `lidx1` → threads per workgroup. 64-256 typical.
- **Accumulators**: `acc0[N]` → N outputs per thread (UPCAST=N).
- **Reduction loops**: `for (int Ridx...)` → serial work per thread. Long loops = missing GROUPTOP.
- **Byte reads**: `unsigned char val = *(data+...)` → quantized dequant, inherently scattered.
- **Threadgroup memory**: `threadgroup float` → GROUPTOP/GROUP is active (good).

Red flags:
- **< 20 GB/s bandwidth**: Too few workgroups OR scattered memory access
- **No threadgroup reduction**: Each thread does full reduction alone. Missing GROUPTOP/GROUP.
- **UPCAST but no GROUP**: Thread writes N elements but reads the full reduction serially.

### Understanding Kernel Optimization (heuristics.py)

tinygrad's optimizer (`tinygrad/codegen/opt/heuristic.py`) applies opts in priority order:

1. **Tensor Cores** (`USE_TC > 0`) — skipped for most quantized kernels
2. **Matvec (MV)** — detects `MUL(INDEX, INDEX)` pattern, applies GROUP+LOCAL+UPCAST
   - Controlled by `MV_BLOCKSIZE=4`, `MV_THREADS_PER_ROW=8`, `MV_ROWS_PER_THREAD=4`
   - **Fails for fused dequant+matmul** because pattern is `MUL(dequant_chain, INDEX)` not `MUL(INDEX, INDEX)`
3. **GROUPTOP** — threadgroup-cooperative reduction, only if output upcast dims <= 2048, only size 16
4. **Masked UPCAST** — for WHERE-gated axes <= 7
5. **More UPCAST** — aggressive, while upcast_size < 32
6. **UNROLL** — last reduce dim if <= 32
7. **LOCAL** — assign threads to output dims, up to 128 total

Why fused dequant MoE kernels are slow: the MV heuristic requires `MUL(INDEX, INDEX)`. Fused dequant inlines bitwise ops between INDEX and MUL, breaking pattern match. Falls through to generic opts with no GROUP = serial reduction at ~2 GB/s.

### Available Models
See `models` dict in llm.py for full list:
- Llama 3.2 (1B, 3B), Llama 3.1 (8B)
- Qwen3 (0.6B, 1.7B, 8B, 30B-A3B MoE)
- OLMoE (1B-7B)
- GLM-4.7-Flash
- DeepSeek-V2-Lite
- Youtu-LLM (2B)

### Running the Smoke Test
```bash
python tinygrad/apps/smoke_test.py
```
Tests youtu-llm, deepseek-v2-lite, and glm-4.7:flash with expected token outputs.

## Q4K Kernel Performance Reference

### Benchmark Results (4096x4096, batch=1)

| Kernel | Per-kernel | Bandwidth | Notes |
|--------|------------|-----------|-------|
| q4k_linear_msl (warm) | **65us** | **145 GB/s** | Best after GPU warmup |
| q4k_linear_beam | 136us | 70 GB/s | Works with or without BEAM |
| q4k_linear_msl (cold) | 155us | 61 GB/s | Before GPU cache warms up |
| q4k_linear_uop | 1060us | 8.9 GB/s | UOp.special() - BEAM cannot help |
| q4k_linear_tensor | 5400us | 1.7 GB/s | Many unfused kernels |

### Key Insights
- **GPU warmup matters**: MSL kernel goes from 155us (cold) to 65us (warm) — 2x from GPU cache effects
- **UOp.special() vs UOp.range()**: `UOp.special()` fixes parallelism (BEAM can't optimize), `UOp.range()` lets BEAM tile and optimize (70 GB/s with proper opts)
- **Pure tensor ops don't fuse for Q4K**: Bit manipulation prevents fusion, generates ~45 separate kernels

### Cross-Model Performance Comparison

| Model | Params | Steady-state | ms/tok | Scaling |
|-------|--------|--------------|--------|---------|
| youtu-llm:2b-Q4 | 0.69 GB | 55 tok/s | 18ms | 1x |
| deepseek-v2-lite | ~2 GB | 20 tok/s | 50ms | 2.8x |
| glm-4.7:flash | ~7 GB | 18 tok/s | 56ms | 3.1x |

**Key finding**: glm4 is 10x more params but only ~3x slower — kernel count/dispatch overhead dominates, not memory bandwidth.

### Where Time Goes (youtu-llm:2b-Q4 profile)

| Category | % of Token Time | Notes |
|----------|----------------|-------|
| q4k_linear | 25% | Expert matmuls (already optimized) |
| Elementwise | 40% | Many small E_* kernels, launch-overhead dominated |
| Reductions | 20% | Attention, MoE weighted sums |
| Other | 15% | Gating, routing, misc |

**Root cause**: ~400 kernels/token x ~20us launch overhead = 8ms/token just in dispatch. The real bottleneck is kernel count, not individual kernel efficiency.

### MoE Performance Bottlenecks

For MoE models (deepseek-v2-lite, glm-4.7:flash):
1. **MoE output combination is #1 bottleneck** (30% of token time): `expert_output * probs + sum`
2. **Tiny gating kernels** (E_32_2, E_16_2_2n1, etc.) from topk/bitonic sort: ~500 calls/token
3. **SIMPLE_TOPK** (iterative argmax) reduces kernel count 27% but same tok/s at this scale

### Benchmarking Rules

**NEVER run multiple GPU benchmarks in parallel.** They contest GPU memory and will lock up the machine. Always run benchmarks sequentially, one at a time.

Behavioral guidelines to reduce common LLM coding mistakes. Merge with project-specific instructions as needed.

Tradeoff: These guidelines bias toward caution over speed. For trivial tasks, use judgment.
1. Think Before Coding

Don't assume. Don't hide confusion. Surface tradeoffs.

Before implementing:

    State your assumptions explicitly. If uncertain, ask.
    If multiple interpretations exist, present them - don't pick silently.
    If a simpler approach exists, say so. Push back when warranted.
    If something is unclear, stop. Name what's confusing. Ask.

2. Simplicity First

Minimum code that solves the problem. Nothing speculative.

    No features beyond what was asked.
    No abstractions for single-use code.
    No "flexibility" or "configurability" that wasn't requested.
    No error handling for impossible scenarios.
    If you write 200 lines and it could be 50, rewrite it.

Ask yourself: "Would a senior engineer say this is overcomplicated?" If yes, simplify.
3. Surgical Changes

Touch only what you must. Clean up only your own mess.

When editing existing code:

    Don't "improve" adjacent code, comments, or formatting.
    Don't refactor things that aren't broken.
    Match existing style, even if you'd do it differently.
    If you notice unrelated dead code, mention it - don't delete it.

When your changes create orphans:

    Remove imports/variables/functions that YOUR changes made unused.
    Don't remove pre-existing dead code unless asked.

The test: Every changed line should trace directly to the user's request.
4. Goal-Driven Execution

Define success criteria. Loop until verified.

Transform tasks into verifiable goals:

    "Add validation" → "Write tests for invalid inputs, then make them pass"
    "Fix the bug" → "Write a test that reproduces it, then make it pass"
    "Refactor X" → "Ensure tests pass before and after"

For multi-step tasks, state a brief plan:

1. [Step] → verify: [check]
2. [Step] → verify: [check]
3. [Step] → verify: [check]

Strong success criteria let you loop independently. Weak criteria ("make it work") require constant clarification.